---
title: "7-Machine learning"
format:
  revealjs: 
    theme: [dark, custom.scss] 
editor: visual
page-layout: full
author: "Thibaut FABACHER"
institute: "GMRC"
incremental: true
slide-number: h/v
progress: true
footer: "Master Intelligence des donnéees de santé / UE Technique"
show-slide-number: all
jupyter: false
cap-location: bottom
self-contained: true
execute:
  echo: true
---

<!-- #\| output-location: column-fragment -->

# Algorithmes de machine learning

------------------------------------------------------------------------

## **Linear Regression**

-   Used for predicting a continuous value

-   Example: predicting housing prices

-   Formulas:

-   $y = mx + b$ (where $y$ is the predicted value, $x$ is the input feature, $m$ is the slope, and $b$ is the y-intercept)

-   Mean Squared Error (MSE) loss function: $\frac{1}{n}\sum_{i=1}^n(y_i - \hat{y_i})^2$

------------------------------------------------------------------------

## **Logistic Regression**

-   Used for predicting a binary outcome (e.g. yes/no, pass/fail)

-   Example: predicting whether a customer will make a purchase

-   Formulas:

-   sigmoid function: \$ \frac{1}{1 + e^{-z}}\$ (where $z$ is the input to the function)

-   Cross-Entropy loss function: $-\sum_{i=1}^n y_i log(\hat{y_i}) + (1-y_i)log(1-\hat{y_i})$

------------------------------------------------------------------------

## **Decision Tree**

-   Used for both classification and regression

-   Example: predicting if a customer will churn

-   Formulas:

-   Information Gain: $IG(D_{p},f) = I(D_{p})-\sum_{j=1}^{m} \frac{N_j}{N_p}I(D_j)$

-   Gini Index : $Gini(D)= 1-\sum_{i=1}^{c}p_i^2$

------------------------------------------------------------------------

## **Random Forest**

-   Used for both classification and regression

-   Example: predicting if a customer will default on a loan

-   Formulas:

-   Bootstrap Aggregating (Bagging)

-   Random Subspace Method (RSM)

------------------------------------------------------------------------

## **Support Vector Machine (SVM)**

-   Used for classification

-   Example: detecting spam emails

-   Formulas:

-   Linear SVM : $y = wx + b$

-   Non-Linear SVM : $y = \sum_{i=1}^{n} \alpha_i y_i K(x_i,x) + b$

------------------------------------------------------------------------

## **Neural Network**

-   Used for both classification and regression, as well as unsupervised learning

-   Example: image recognition

-   Formulas:

-   Feedforward: $a = f(Wx + b)$

-   Backpropagation: $\frac{\partial L}{\partial w} = \frac{\partial L}{\partial a} * \frac{\partial a}{\partial w}$

------------------------------------------------------------------------

## **K-means Clustering**

-   Used for unsupervised learning

-   Example: customer segmentation

-   Formulas:

-   Distance: $d(x,y) = \sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}$

-   Centroid: $\mu_j = \frac{\sum_{i=1}^{n} 1_{c_i = j} x_i}{\sum_{i=1}^{n} 1_{c_i = j}}$

------------------------------------------------------------------------

## **Principal Component Analysis (PCA)**

-   Used for unsupervised learning

-   Example: dimensionality reduction

------------------------------------------------------------------------

## **XGBoost**

-   XGBoost stands for "eXtreme Gradient Boosting"

-   It is an ensemble learning method for classification and regression problems

-   XGBoost is based on decision trees

-   XGBoost algorithm can handle missing values and can be used for feature selection

------------------------------------------------------------------------

## Regression with regulation

-   

    ## $\beta = argmin_{\beta} \frac{1}{n}||Y - X\beta||_2^2 + \lambda_1||\beta||_1 + \frac{\lambda_2}{2}||\beta||_2^2$

# Arbre de decision

<https://mlu-explain.github.io/decision-tree/>

-   algorithme d'apprentissage supervisé

-   tâches de classification et de régression

-   nœud interne de l'arbre représente une caractéristique ou une propriété,

-   chaque nœud feuille représente une étiquette de classe ou une valeur

-   construction en divisant récursivement en fonction de la caractéristique qui donne les sous-ensembles les plus purs

# Construction Arbre de decision

-   en partant de la racine, qui représente l'ensemble des données d'entraînement

-   on choisit la caractéristique qui divise le mieux les données en sous-ensembles purs selon le critère choisi

-   jusqu'à ce qu'un certain critère d'arrêt soit atteint : profondeur max, nobre minnimum de données par noeud

-   feuilles représentent les classes cibles

-   Arbre utiliser pour classifier des nouvelles données

## Critère de séparation des données:

-   L'impureté de Gini : $$ Gini = 1 - \sum_{i=1}^{n}P(i|t)^2 $$

- $P(i|t)$ : la probabilité d'appartenance à la classe $i$ pour les données dans le nœud $t$.

L'impureté de Gini : probabilité qu'un élément choisi au hasard dans le nœud soit mal classé.

## Critère de séparation des données:

-   L'entropie :

$$ H(S) = -\sum_{i=1}^{n}P(i|S)log(P(i|S)) $$

- $P(i|S)$ : la probabilité d'appartenance à la classe $i$ pour les données dans l'ensemble $S$. L'entropie : incertitude des données dans l'ensemble.

## Exemple visuel

![](images/paste-29D428D9.png)

------------------------------------------------------------------------

![](images/paste-CC4E82B9.png)

------------------------------------------------------------------------

![](images/paste-2C2BE9F7.png)

------------------------------------------------------------------------

![](images/paste-DFD335AC.png)

------------------------------------------------------------------------

## Exemple arbre de décision

| couleur | poids (kg) | nombre de pépins | type      |
|---------|------------|------------------|-----------|
| rouge   | 0.5        | 5                | fraise    |
| rouge   | 0.8        | 10               | framboise |
| jaune   | 0.3        | 2                | cerise    |
| vert    | 0.7        | 8                | pomme     |
| vert    | 0.6        | 3                | poire     |

## Exemple arbre de décision

::: {style="font-size : 10px;"}
| couleur | poids (kg) | nombre de pépins | type      |
|---------|------------|------------------|-----------|
| rouge   | 0.5        | 5                | fraise    |
| rouge   | 0.8        | 10               | framboise |
| jaune   | 0.3        | 2                | cerise    |
| vert    | 0.7        | 8                | pomme     |
| vert    | 0.6        | 3                | poire     |
:::

1.  On calcule l'entropie pour chaque propriété pour chaque propriété (couleur, poids, nombre de pépins) en utilisant les données de l'ensemble.

--------------

::: {style="font-size : 10px;"}
| couleur | poids (kg) | nombre de pépins | type      |
|---------|------------|------------------|-----------|
| rouge   | 0.5        | 5                | fraise    |
| rouge   | 0.8        | 10               | framboise |
| jaune   | 0.3        | 2                | cerise    |
| vert    | 0.7        | 8                | pomme     |
| vert    | 0.6        | 3                | poire     |
:::

$$ Gini(base) = 1-1/5^2-1/5^2-1/5^2-1/5^2-1/5^2 $$

$$ Gini(couleur) = 1 - \sum_{i=1}^{n}P(i|couleur)^2  $$

$$ Gini(poids) = 1 - \sum_{i=1}^{n}P(i|poids)^2 $$

$$ Gini(nombre\ de\ pépins) = 1 - \sum_{i=1}^{n}P(i|nombre\ de\ pépins)^2 $$

## Calcul pour couleur :

$$ Gini(rouge) = 1 - \sum_{i=1}^{n}P(i|rouge)^2 = 1 - 0^2 - 0^2-0.5^2-0.5^2 = 0.5 $$

$$ Gini(jaune) = 1 - \sum_{i=1}^{n}P(i|rouge)^2 =\\ 1 - 0^2 - 0^2-1^2-0^2 = 1 $$ $$ Gini(vert) = 1 - \sum_{i=1}^{n}P(i|rouge)^2 = 1 - 0.5^2 - 0^2-0.5^2-0^2 = 0.75 $$ 
$$ Impurete\_moyenne = 2/5*Gini(rouge)+1/5*Gini(ver)+2/5*Gini(jaune) = 0.7 $$ 

## Calcul pour nombre de pépins :

- $$ Gini(nbpepin>4) = 1 - \sum_{i=1}^{n}P(i|nbpepin>4)^2 =\\ 1 -0^2- 0^2 - 1/3^2-1/3^2-1/3^2 = 0.66 $$

- $$ Gini(nbpep\leq4) = 1 - \sum_{i=1}^{n}P(i|\leq4)^2 = \\1 -1/2^2- 1/2^2 - 0^2-0^2-0^2 = 0.75 $$ 

- $$Gini(nb\_pep) = 2/3*0.66+1/3*0.75 = 0.69 $$

------------

2.  propriété qui minimise l'impureté de Gini pour séparation : $nb_pep \leq4$.

3.  On crée deux sous-nœuds pour les fruits avec plus ou moins de 4 pépins

4.  On répète les étapes 1 à 3 pour chaque sous-nœud en utilisant uniquement les données associées à ce sous-nœud. Exemple, $$ Gini(poids | nbpep\leq4) = 1 - \sum_{i=1}^{n}P(i|poids,nbpep\leq4)^2 $$

5.  On répète jusqu'au critère de fin

## Hyperparamètres arbre de decision

-   Criterion : entropy ou gini

-   splitter : best ou random

-   Profondeur maximale : $Max\_depth$

-   Nombre minimum d'éléments dans un noeud de feuille : $Min\_samples\_leaf$

-   Nombre maximum de feuilles : $Max\_leaf\_nodes$

-   Nombre minimum de split : $Min\_samples\_split$

## Pruning arbre :

-   Pré-élagage : règle d'arrêt évaluer à chaque nouveau nœud qui stoppe la construction de l'arbre -\> méthode de haut en bas

-   Post-élagage : Construction de l'arbre dans son intégralité puis calcul de l'intérêt de chaque nœud en partant des nœud terminaux -\> méthode de bas en haut

## Avantages inconvenients :

::: columns
::: {.column width="50%"}
\- Compréhensible par un humain

\- Pas besoin de normaliser les données

\- Accepte les données quanti et quali
:::

::: {.column width="50%"}
Pb de surrapprentissage

Pb avec les classifications non équilibrés
:::
:::

------------------------------------------------------------------------

![](images/paste-AED8BB0A.png)

## Implémentation R et Python

-   r : rpart https://ouvrir.passages.cnrs.fr/arbre_decision/\_book/pratique.html

-   python : sklearn.tree.DecisionTreeClassifier

## Régression logistique

<https://mlu-explain.github.io/logistic-regression/>

-   Y binaire

-   Modélisation de $$ P(Y=1) $$

-   $P(y=1|x) = \frac{1}{1 + e^{-(b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n)}}$

-   Fonction sigmoïdale comprise entre 0 et 1

![](images/paste-1E517856.png){width="360"}

## Régression linéaire

-   Y quantitative

- $y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n$

## Regression pénalisée

- Ridge : 
$SSE_{ridge} = \sum_{i=1}^{n} (y_i - \beta^TX_i)^2 + \lambda \beta^T\beta$


$\sum_{i=1}^n (y_i - \hat{y}i)^2 + \lambda \sum{j=1}^p |\beta_j|$
